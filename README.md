# text-classification
模型链接：https://pan.baidu.com/s/1A1OL6fUeFA63a_toNpcteA 提取码：71cw
整体的代码架构分为五个部分NEZHA、torch_utils、preprocess、postprocess以及核心代码文件。

NEZHA文件夹：NEZHA预训练模型是当前泛化性能比较优秀的中文预训练模型。为此，本书将使用NEZHA预训练模型作为本书自然语言处理下游任务的预训练模型。而NEZHA文件夹存放的则是调用NEZHA预训练模型所需要的代码。

torch_utils文件夹：存放一些自然语言处理下游任务所用到的第三方代码文件。

preprocess文件夹：存放数据预处理的代码文件。

postprocess文件夹：有时，我们需要对模型预测出来的结果进行后处理，该文件夹则是存放对模型结果进行后处理的代码文件。

核心代码文件：

config.py：自然语言处理任务的所有路径与超参数的设置均在于此。

model.py：模型构建的代码文件。

utils.py：构建模型输入所需要的数据迭代器，通过构建数据迭代器，我们可以将数据按照批次（batch size）喂入模型，用以模型训练、验证与预测。

optimization.py：构建模型训练所需要的优化器。

train_fine_tune.py：模型训练与验证的代码文件。该代码文件通过数据迭代器utils.py将训练集喂入模型进行训练，并在每一轮迭代（epoch）之后，利用验证集与评估指标对当前模型进行评估与保存。

predict.py：对测试集进行预测。


7.4.1 数据预处理
将每一条文本数据的前128个字与后382个字进行了拼接，以完成超长文本的信息保存，并抛弃冗余信息。

与此同时，由于深度学习模型无法处理文本信息，为此数据预处理代码文件还需将原始数据的文本标签映射成数字标签，以便模型的后续读取。

由于中文预训练模型NEZHA与BERT模型结构相似，为此，本文详细参考了BERT论文的超参数设置，并基于当前任务数据的特性与设备资源（32GB内存的V100显卡），最终在config.py代码文件中设置了一系列的超参数。下面是对config.py代码文件的一些通用超参数介绍，其他基于当前文本分类任务的超参数则会跟随实验流程进行介绍。

sequence length为输入模型的最大文本长度，根据7.1小节中的数据长度进行设置，而batch size则由当前数据的sequence length与16GB内存的Tesla显卡资源共同决定。

warmup proportion则为慢热学习比例，用来保证前面0.05训练步伐的学习率较低，避免模型在训练的开始由于其随机初始化的权重导致的训练震荡，而后学习率再缓慢趋向于之前设置的学习率。

decay rate则是下游网络结构的学习率衰减参数，因为网络结构的学习率随着训练进程的衰减有助于模型更好地拟合数据。

learning rate与embed learning rate分别为下游网络结构的学习率与预训练模型网络结构的学习率，采用分层学习率的原因是预训练模型NEZHA在本文使用之前已经经过大规模无监督语料的预训练，需要设置更小的学习率（5e-5）进行精细微调，而下游网络结构的权重是在训练开始时随机初始化的，则需要相对较大的学习率（1e-4）进行微调。
